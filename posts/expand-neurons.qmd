---
title: "Expand Neurons, Not Parameters"
date: 2025-10-06
description: 'Fixed Parameter Expansion widens networks without increasing non‑zero parameters.  This improves accuracy by reducing superposition/polysemanticity.'
categories: [Superposition, Sparsity]
author:
  - name: Linghao Kong
  - name: Inimai Subramanian
  - name: Yonadav Shavit
  - name: Micah Adler
  - name: Dan Alistarh
  - name: Nir Shavit
abstract: |
  This paper explores how widening a network without increasing the number of non‑zero parameters can improve performance by reducing interference among features in superposition. The authors propose Fixed Parameter Expansion (FPE): replace a neuron with several children and partition the parent’s non‑zero weights disjointly among them. On Boolean‑code tasks, clause‑aligned FPE lowers polysemanticity metrics and raises accuracy.  Furthermore, random weight splits capture much of the benefit, suggesting reduced feature collisions—not precise assignment—is the key driver. Transferring to classifiers over CLIP embeddings and deeper multilayer networks, widening at a fixed non‑zero parameter count consistently boosts accuracy. The results show width is an interpretability‑motivated lever against superposition.  This is well matched with modern accelerators where memory movement of non‑zero parameters is the dominant bottleneck.
---
**Paper:** [arXiv:2510.04500](https://arxiv.org/abs/2510.04500)
